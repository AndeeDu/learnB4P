{
  "hash": "6e10f5f5547d034c2ae0429e77eacb6f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"01 - Introduction\"\nauthor: \"Stefano Coretta\"\nformat:\n  mono-light-revealjs:\n    theme: [default, custom.scss]\n    history: false\nfilters:\n  - tachyonsextra\nexecute: \n  echo: true\n  \n---\n\n\n\n\n## Schedule: Day 1\n\n| time        | topic                     |\n| ----------- | ------------------------- |\n| 10.00-11.15 | 01 Introduction           |\n| 11.15-11.30 | BREAK                     |\n| 11.30-12.30 | 02 Priors                 |\n| 12.30-13.30 | LUNCH                     |\n| 13.30-14.45 | 03 Categorical predictors |\n| 14.45-15.00 | BREAK                     |\n| 15.00-16.00 | 04 Interactions           |\n| 16.00-17.00 | Q&A                       |\n\n## Schedule: Day 2\n\n| time        | topic                   |\n| ----------- | ----------------------- |\n| 10.00-11.15 | 05 Numeric predictors   |\n| 11.15-11.30 | BREAK                   |\n| 11.30-12.30 | 06 Multilevel effects   |\n| 12.30-13.30 | LUNCH                   |\n| 13.30-14.45 | 07 Practice             |\n| 14.45-15.00 | BREAK                   |\n| 15.00-16.00 | 08 Sensitivity analysis |\n| 16.00-17.00 | Q&A                     |\n\n## Statistical modelling\n\n![From McElreath 2019](img/models.png)\n\n## Statistical inference\n\n![The inference process](img/inference.png)\n\n\n## Two frameworks of inference\n\n::: {layout-ncol=\"2\"}\n![](img/np.jpg){fig-align=\"center\" width=\"248\"}\n\n![](img/Thomas_Bayes.gif){fig-align=\"center\" width=\"373\"}\n:::\n\n\n## Frequentist framework\n\n::: box-note\n- Neyman-Pearson's Null Hypothesis Significance Testing (NHST).\n\n- Based on frequentist interpretation of probabilities as **long-run occurrences of events**.\n\n- Output:\n  - **Point estimates** of predictors' parameters (with standard error).\n  -   *p*-values.\n:::\n\n\n## Bayesian framework\n\n::: box-note\n- Bayesian inference.\n\n- Based on the Bayesian interpretation of probabilities as **degree of belief in the occurrence of events**.\n\n-  Output:\n  - **Probability distributions** of model's parameters.\n:::\n\n\n## Bayesian regression: basics\n\n::: box-note\n-   R package brms.\n\n-   Same syntax as lme4.\n\n-   One function to fit many types of regression models: `brm()`.\n:::\n\n\n## Example data: background\n\n::: box-note\n- Effect of consonant voicing on preceding vowel duration in Italian and Polish.\n\n- pVCV words (/a, o, u/, /t, d, k, g/).\n\n- 11 Italian speakers, 6 Polish speakers. 5 repetitions.\n:::\n\n## Example data: the data frame\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(coretta2018itapol)\ndata(\"token_measures\")\n\ntoken_measures\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,343 × 53\n   index speaker file     rec_date        ipu   prompt word    time sentence_ons\n   <dbl> <chr>   <chr>    <chr>           <chr> <chr>  <chr>  <dbl>        <dbl>\n 1     1 it01    it01-001 29/11/2016 15:… ipu_1 Dico … pugu   0.990        0.990\n 2     2 it01    it01-002 29/11/2016 15:… ipu_2 Dico … pada   3.62         0.502\n 3     3 it01    it01-003 29/11/2016 15:… ipu_3 Dico … poco   6.13         0.697\n 4     4 it01    it01-004 29/11/2016 15:… ipu_4 Dico … pata   8.82         0.623\n 5     5 it01    it01-005 29/11/2016 15:… ipu_5 Dico … boco  11.5          0.665\n 6     6 it01    it01-006 29/11/2016 15:… ipu_6 Dico … podo  14.3          0.647\n 7     7 it01    it01-007 29/11/2016 15:… ipu_7 Dico … boto  17.2          0.740\n 8     8 it01    it01-008 29/11/2016 15:… ipu_8 Dico … paca  19.7          0.502\n 9     9 it01    it01-009 29/11/2016 15:… ipu_9 Dico … bodo  22.3          0.556\n10    10 it01    it01-010 29/11/2016 15:… ipu_… Dico … pucu  24.8          0.535\n# ℹ 1,333 more rows\n# ℹ 44 more variables: sentence_off <dbl>, word_ons <dbl>, word_off <dbl>,\n#   v1_ons <dbl>, c2_ons <dbl>, v2_ons <dbl>, c1_rel <dbl>, c2_rel <dbl>,\n#   voicing_start <dbl>, voicing_end <dbl>, voicing_duration <dbl>,\n#   voiced_points <dbl>, GONS <dbl>, max <dbl>, NOFF <dbl>, NONS <dbl>,\n#   peak1 <dbl>, peak2 <dbl>, c1_duration <dbl>, c1_clos_duration <dbl>,\n#   c1_vot <dbl>, c1_rvoff <dbl>, v1_duration <dbl>, c2_duration <dbl>, …\n```\n\n\n:::\n:::\n\n\n## Vowel duration and consonant voicing\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_introduction_files/figure-revealjs/vdur-strip-1.png){width=960}\n:::\n:::\n\n\n## Vowel duration\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_introduction_files/figure-revealjs/vdur-dens-1.png){width=960}\n:::\n:::\n\n\n## Gaussian model of vowel duration\n\n$$\n\\begin{align}\ndur & \\sim Gaussian(\\mu, \\sigma)\n\\end{align}\n$$\n\n-   $dur$ is vowel duration\n\n-   $\\sim$ means \"is distributed/generated according to\".\n\n-   $Gaussian(\\mu, \\sigma)$ a Gaussian distribution with mean $\\mu$ and standard deviation $\\sigma$.\n\n## Fit the model with brms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_1 <- brm(\n  v2_duration ~ 1,\n  family = gaussian,\n  data = token_measures,\n  cores = 4,\n  file = \"data/cache/m_1\",\n)\n```\n:::\n\n\n## Model summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: v2_duration ~ 1 \n   Data: token_measures (Number of observations: 1342) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    78.10      0.84    76.45    79.78 1.00     2657     2561\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    31.33      0.60    30.19    32.54 1.00     3320     2578\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Model summary: 80% CrIs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m_1, prob = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: v2_duration ~ 1 \n   Data: token_measures (Number of observations: 1342) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept    78.10      0.84    77.02    79.17 1.00     2657     2561\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma    31.33      0.60    30.57    32.10 1.00     3320     2578\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Plot posterior probability distributions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(m_1, combo = c(\"dens\", \"trace\"))\n```\n\n::: {.cell-output-display}\n![](01_introduction_files/figure-revealjs/m-1-plot-1.png){width=960}\n:::\n:::\n\n\n## Diagnostics: posterior predictive checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(m_1, ndraws = 50)\n```\n\n::: {.cell-output-display}\n![](01_introduction_files/figure-revealjs/m-1-pp-check-1.png){width=960}\n:::\n:::\n\n\n## Log-normal model of vowel duration\n\n$$\n\\begin{align}\ndur & \\sim LogNormal(\\mu, \\sigma)\n\\end{align}\n$$\n\n. . .\n\n$$\n\\begin{align}\nlog(dur) & \\sim Gaussian(\\mu_1, \\sigma_1)\n\\end{align}\n$$\n\n## Fit the log-normal model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_2 <- brm(\n  v2_duration ~ 1,\n  family = lognormal,\n  data = token_measures,\n  cores = 4,\n  file = \"data/cache/m_2\",\n)\n```\n:::\n\n\n## Model summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m_2, prob = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: v2_duration ~ 1 \n   Data: token_measures (Number of observations: 1342) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept     4.30      0.01     4.28     4.31 1.00     3405     2635\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.34      0.01     0.33     0.35 1.00     2798     2230\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n## Posterior predictive checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(m_2, ndraws = 50)\n```\n\n::: {.cell-output-display}\n![](01_introduction_files/figure-revealjs/m-2-pp-check-1.png){width=960}\n:::\n:::\n",
    "supporting": [
      "01_introduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}